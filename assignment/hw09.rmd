---
title: "HW 9"
output: pdf_document
---
# Intro to Data Science HW 9
##### Copyright Jeffrey Stanton, Jeffrey Saltz, and Jasmina Tacheva


```{r}
# Enter your name here: Chaithra Kopparam
```

### Attribution statement: (choose only one and delete the rest)


```{r}
# 1. I did this homework by myself, with help from the book and the professor.
```

Supervised learning means that there is a **criterion one is trying to predict**. The typical strategy is to **divide data** into a **training set** and a **test set** (for example, **two-thirds training** and **one-third test**), train the model on the training set, and then see how well the model does on the test set. <br>

**Support vector machines (SVM)** are a highly flexible and powerful method of doing **supervised machine learning**.

Another approach is to use **partition trees (rpart)** 

In this homework, we will use another banking dataset to train an SVM model, as well as an rpart model, to **classify potential borrowers into 2 groups of credit risk** – **reliable borrowers** and **borrowers posing a risk**. You can learn more about the variables in the dataset here:<br> https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 <br>

This kind of classification algorithms is used in many aspects of our lives – from credit card approvals to stock market predictions, and even some medical diagnoses. <br>

## Part 1: Load and condition the data  

A.	Read the contents of the following .csv file into a dataframe called **credit**: <br>

https://intro-datascience.s3.us-east-2.amazonaws.com/GermanCredit.csv <br>

You will also need to install( ) and library( ) several other libraries, such as **kernlab** and **caret**.



```{r}
library(kernlab)
library(caret)
library(tidyverse)
credit <- read_csv("https://intro-datascience.s3.us-east-2.amazonaws.com/GermanCredit.csv")
```

B.	Which variable contains the outcome we are trying to predict, **credit risk**? For the purposes of this analysis, we will focus only on the numeric variables and save them in a new dataframe called **cred**:


```{r}
# yes, credit risk is the outcome variable we are trying to predict
cred <- data.frame(duration=credit$duration, 
                   amount=credit$amount, 
                   installment_rate=credit$installment_rate, 
                   present_residence=credit$present_residence, 
                   age=credit$age, 
                   credit_history=credit$number_credits, 
                   people_liable=credit$people_liable, 
                   credit_risk=as.factor(credit$credit_risk))
```
C.	Although all variables in **cred** except **credit_risk** are coded as numeric, the values of one of them are also **ordered factors** rather than actual numbers. In consultation with the **data description link** from the intro, write a comment identifying the **factor variable** and briefly **describe** each variable in the dataframe. 


```{r}
# credit_history is coded as numeric but it can be factored with below values
# 1- no credits taken/all credits paid back duly
# 2- all credits at this bank paid back duly
# 3- existing credits paid back duly till now
# 4- delay in paying off in the past
# 5- critical account/other credits existing (not at this bank)

# briefly describe each variable in the dataframe
#duration: duration in months
#amount: credit amount
#installment_rate: Installment rate in percentage of disposable income
#present_residence: Present residence since
#age: age of the customer in years
#credit history: credit history of the customer with above mentioned values
#people_liable: Number of people being liable to provide maintenance for
#credit_risk: indictes potential borrowers
# 0- reliable borrowers
# 1- borrowers posing a risk
```

## Part 2: Create training and test data sets

A.	Using techniques discussed in class, create **two datasets** – one for **training** and one for **testing**.


```{r}
set.seed(100)
# randomly sample for training dataset elements
# 40% of data from subCredit = 400 cases will used for building the model
trainList <- createDataPartition(y=cred$credit_risk,p=.40,list=FALSE)
training <- cred[trainList,]
testing <- cred[-trainList,]
```

B.	Use the dim( ) function to demonstrate that the resulting training data set and test data set contain the appropriate number of cases.


```{r}
dim(training) # must have 400 rows
dim(testing) # must have 600 rows
```

## Part 3: Build a Model using SVM

A.	Using the caret package, build a support vector model using all of the variables to predict **credit_risk**


```{r}
svmModel <- ksvm(credit_risk~.,data=training,C=5,cross=3,prob.model=TRUE)
```

B. output the model

Hint: explore finalModel in the model that would created in F.


```{r}
svmModel
# cross validation error is approx 33.24%
# This indicates overall prediction error of all the 3 folds (cross=3)

# considering cross validation error, the model has accuracy
# of ~66.76% (100% - (cross validaton error%)) on training data set
```

## Part 4: Predict Values in the Test Data and Create a Confusion Matrix

A.	Use the predict( ) function to validate the model against test data. Store the predictions in a variable named **svmPred**.


```{r}
svmPred <- predict(svmModel,newdata=testing, type="response")
```

B.	The **svmPred** object contains a list of classifications for reliable (=0) or risky (=1) borrowers. Review the contents of **svmPred** using head( ).


```{r}
head(svmPred) # svmPred is a vector of factor variable with two reliable borrowers(0)/borrowers
# posing risk (1)
```

C.	Explore the **confusion matrix**, using the caret package


```{r}
confusionMatrix(svmPred,testing$credit_risk)
```

D.	What is the **accuracy** based on what you see in the confusion matrix.

```{r}
#accuracy is 69.67%
```

E.	Compare your calculations with the **confusionMatrix()** function from the **caret** package.

```{r}
str(svmPred)
table(svmPred,testing$credit_risk)

accuracy <- sum(diag(table(svmPred,testing$credit_risk)))/sum(table(svmPred,testing$credit_risk))
accuracy

# diagonal elements indicate the right prediction of both values of credit_risk
# off-diagonal elements are the wrong predictions
# so, accuracy = sum of all the right predictions/total predictions

# accuracy is same as that was calculated in confusion matrix
```

F.	Explain, in a block comment:<br> 1) why it is valuable to have a “test” dataset that is separate from a “training” dataset, and <br>2) what potential ethical challenges this type of automated classification may pose. 


```{r}
# 1) why it is valuable to have a “test” dataset that is separate from a “training” dataset
# to check if the model is overfitting. We can get accuracy of the model as 90% with training
# data set but, when it starts to predict values for the data it that model has never seen
# accuracy can be 20%.

# 2) what potential ethical challenges this type of automated classification may pose.
# while partitioning the data, we do not know how the data is being divided as training and testing sets
# we need to balance the classification with respect to outcome
# variable otherwise, data will not be representative of the population
# also, it can lead to bias in the data
```

## Part 5: Now build a tree model (with rpart)

A. Build a model with rpart
<br>
Note: you might need to install the e1071 package


```{r}
#install.packages("rpart")
library(rpart)
creditTree <- rpart(credit_risk~.,data=training)
```

B. Visualize the results using  rpart.plot()


```{r}
library(rpart.plot)
prp(creditTree,faclen = 0, cex=0.8,extra = 1)
```

C. Use the **predict()** function to predict the testData, and then generate a confusion matrix to explore the results

```{r}
predictValues <- predict(creditTree, newdata=testing, type = "class")
# confusion matrix
table(predictValues,testing$credit_risk)

accuracy <- sum(diag(table(predictValues,testing$credit_risk)))/
  sum(table(predictValues,testing$credit_risk))
accuracy
```

D. Review the attributes being used for this credit decision. Are there any that might not be appropriate, with respect to fairness? If so, which attribute, and how would you address this fairness situation. Answer in a comment block below

```{r}
# Are there any that might not be appropriate, with respect to fairness?
# yes, present_residence attribute might not be appropriate in
# predicting credit risk
# present residence : how long the person is staying in particular address will not
# affect whether he/she can pay the installments on time.

# how would you address this fairness situation?
# by excluding the attribute while building the model
```
