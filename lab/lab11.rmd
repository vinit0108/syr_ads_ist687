---
title: "Week 11 Lab – Text Mining"
output: pdf_document
---

Instructions: Text mining plays an important role in many industries because of the prevalence of text in the interactions between customers and company representatives. Even when the customer interaction is by speech, rather than by chat or email, speech to text algorithms have gotten so good that transcriptions of these spoken word interactions are often available. To an increasing extent, a data scientist needs to be able to wield tools that turn a body of text into actionable insights.

In this exercise, we work with a small number of social media posts on the topic of climate change. Make sure to install and library the readr and quanteda package before starting the exercise. Please include an attribution statement (see syllabus).
```{r}
library(tidyverse)
install.packages("quanteda")
library(quanteda)
install.packages("quanteda.textstats")
install.packages("quanteda.textplots")
library(quanteda.textstats)
library(quanteda.textplots)
```
1.	Read in the following data set with read_csv():
https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv

The name of the file is “ClimatePosts.csv”. Store the data in a data frame called tweetDF. Use str(tweetDF) to summarize the data. Add a comment describing what you see. Make sure to explain what each of the three variables contains.
```{r}
tweetDF <- read_csv("https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv")
str(tweetDF)
```
2.	Use the corpus commands to turn the text variable into a quanteda corpus. You can use the IDs as the document titles with the following command:
tweetCorpus <- corpus(tweetDF$Tweet, docnames=tweetDF$ID)
```{r}
tweetCorpus <- corpus(tweetDF$Tweet, docnames=tweetDF$ID)
```
3.	Next, convert the corpus into a document-feature matrix (DFM).  Before you do that you can use “tokens” to remove punctuation and stop words. Use this code:
toks <- tokens(tweetCorpus, remove_punct=TRUE)
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")

Here’s a command that will create the DFM:
tweetDFM <- dfm(tweetCorpustoken, tolower = TRUE )
```{r}
toks <- tokens(tweetCorpus, remove_punct=TRUE)
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")

tweetDFM <- dfm(toks_nostop, tolower = TRUE )
```
4.	Type tweetDFM at the console to find out the basic characteristic of the DFM (the number of terms, the number of documents, and the sparsity of the matrix). Write a comment describing what you observe.
```{r}
tweetDFM
```
5.	Create a wordcloud from the DFM using the following command. Write a comment describing notable features of the wordcloud:
textplot_wordcloud(tweetDFM, min_count = 1)
```{r}
textplot_wordcloud(tweetDFM, min_count = 1)
```
6.	Using textstat_frequency() from the quanteda.textstats package, show the 10 most frequent words, and how many times each was used/mentioned.

```{r}
textstat_frequency(x=tweetDFM, n=10)
```
7.	Next, we will read in dictionaries of positive and negative words to see what we can match up to the text in our DFM. Here’s a line of code for reading in the list of positive words:

URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]

Create a similar line of code to read in the negative words, with the following URL: https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt

There should be 2006 positive words and 4783 negative words.
```{r}
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]

URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt"
negWords <- scan(URL, character(0), sep = "\n")
negWords <- negWords[-1:-34]

```
8.	Explain what the following lines of code does and comment each line. Then add similar code for the negative words.
posDFM <- dfm_match(tweetDFM, posWords)
posFreq <- textstat_frequency(posDFM)
```{r}
posDFM <- dfm_match(tweetDFM, posWords)
posFreq <- textstat_frequency(posDFM)

negDFM <- dfm_match(tweetDFM, negWords)
negFreq <- textstat_frequency(negDFM)
```
9.	Explore posFreq and negFreq using str() or glimpse(). Explain the fields in these data frames.
```{r}
glimpse(posFreq)
glimpse(negFreq)
```
10.	 Output the 10 most frequently occurring positive and negative words
including how often each occurred.
```{r}
posFreq %>% slice(1:10)
negFreq %>% slice(1:10)
```
